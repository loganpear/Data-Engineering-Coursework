# Day 1: Fact Data Modeling Notes  

## **What is a Fact?**  
- You can't break it down into a smaller piece.  

### **Examples:**  
- A user logs into an app.  
- A transaction is made.  
- You log in to Instagram 30 times today.  

### **Key Characteristics:**  
- **Facts are NOT slowly changing**—you can't change the past.  
- **More abundant than dimensions**—typically **10-100x more facts** than dimensions.  
- **Context matters**—facts need context to be meaningful.  
  - Example: Sending a notification means little without context, but clicking something, receiving a notification, and then making a purchase is valuable.  
- **Duplicates**—way more common in fact data than dimension data.  

---

## **Normalized vs. Denormalized Facts**  

### **Normalized Facts:**  
- Do **not include dimensional attributes**—just IDs to join for more information.  
- **Smaller scale**—better normalization.  

### **Denormalized Facts:**  
- Include **some dimensional attributes** for **faster analysis**.  
- Trades **more storage** for **speed**.  

---

## **Raw Logs vs. Fact Data**  

### **Raw Logs:**  
- **Ugly schemas**—designed for online systems, making data analysis difficult.  
- **Duplicates and quality errors**—common in raw logs.  
- **Shorter retention periods.**  

### **Fact Data:**  
- **Clean schemas**—with nice column names.  
- **Quality guarantees**—e.g., uniqueness, not null constraints.  
- **Longer retention**—better for long-term analysis.  

---

## **How Does Fact Modeling Work?**  

Think in terms of **Who, Where, When, What, and How**:  

- **Who:** Fields stored as IDs.  
- **Where:** Locations—city, state, or app sections like homepage, profile.  
- **When:** Timestamps—e.g., `event_timestamp`.  
- **What:** Actions—e.g., generated, sent, clicked, delivered.  
- **How:** Often overlaps with **Where**—e.g., "used an iPhone to click."  

---

## **Quality Standards for Fact Datasets:**  
- Should meet **quality guarantees**—otherwise, use raw logs.  
- **No duplicates.**  
- Key fields (**What, When, Who**) should **never be null**.  
- **Smaller size** than raw logs.  
- **Simplified columns**—hard-to-interpret fields should be parsed out.  

---

## **Logging and Fact Data:**  
- **Logs provide context** for facts.  
- **Collaboration** with engineers ensures meaningful logging.  
- **Log only what's needed.**  
- Follow the **online teams' standards** for logging.  
- Tools like **Thrift** are used by companies like **Airbnb** and **Netflix** for logging.  

---

## **Working with High-Volume Fact Data:**  

### **1. Sampling:**  
- **Take a sample** of data to reduce volume.  
- **Best for metric-driven use cases**—where minor imprecision is acceptable.  

### **2. Bucketing:**  
- **Bucket by dimensions** (e.g., user).  
- **Faster joins**—especially useful for high-volume data.  
- **Sorted-merge bucket (SMB) joins**—can avoid shuffling entirely.  

---

## **Retention Period for Fact Data:**  

### **Big Tech Approach:**  
- **< 10 TB:** Retention doesn't matter much.  
- **> 100 TB:** **Very short retention** (~14 days or less).  
- **Anonymization:**  
  - After **60-90 days**, move data to a new table with **PII stripped**.  

---

## **Deduplication of Fact Data:**  
- **Facts often contain duplicates.**  
  - Example: Clicking the same notification multiple times.  

### **Choosing a Deduplication Window:**  
- **No duplicates in...**  
  - A day? An hour? A week?  
- **Analyze duplicate patterns**—distributions help determine the right window.  

### **Intraday Deduplication Methods:**  
- **Streaming:**  
  - Captures most duplicates efficiently.  
- **Microbatch:**  
  - Processes small time windows.  

---

## **Streaming for Deduplication:**  
- **Efficient for short windows.**  
- **Challenges for longer windows:**  
  - Example: Holding a full day's duplicates requires **large memory buffers**.  
- **Key Observations:**  
  - Most duplicates happen **shortly after the first event**.  
  - **15–60 minute windows** work best for deduplication.  

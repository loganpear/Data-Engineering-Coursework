# Day 3: Parallelism, Reduced Fact Data Modeling

---

### Why should shuffle be minimized?
**Shuffle means:** Moving data between nodes or partitions to reorganize it for processing, often during joins, grouping, or sorting operations.  
**Shuffle is the bottleneck for parallel compute since:** It requires network transfers, disk I/O, and serialization, introducing delays and dependencies that disrupt parallel processing efficiency.

---

### Parallelism Levels

**Extremely Parallel:**  
- **`SELECT`, `FROM`, `WHERE`**  
  Running this is basically instant no matter how much data you have.

**Kinda Parallel:**  
- **`GROUP BY:`**  
  It requires data shuffling to group similar values, which involves network and computation overhead.  
  **How can we fix `GROUP BY` causing shuffle?**  
  - **Way 1:** In Spark or S3 or whatever, you can bucket your data based on a key (e.g., `user_id`).  
  - **Way 2:** Reduce the data volume as much as you can.

- **`JOIN:`**  
  It may need to redistribute data across nodes to match keys, introducing dependencies and data movement.

- **`HAVING:`**  
  It applies filters after grouping, relying on the results of `GROUP BY`, which slows down processing.

**Painfully Not Parallel:**  
- **`ORDER BY:`** *(at the end of the query)*  
  Painfully not parallel because it requires sorting all the data across nodes, forcing a global coordination step to arrange rows in the correct order, which eliminates most parallelism and becomes increasingly slow as data size grows.

- **`ORDER BY:`** *(in the context of a window function)*  
  Less painful because it only sorts data within partitions, not across the entire dataset. This allows the sorting to be parallelized within each partition, making it faster and more scalable compared to sorting the entire dataset at the end of a query.

---

### Reduced Fact Data Modeling

Fact data often has this schema at the start:  
```
| user_id | event_time         | action | date_partition |
|---------|---------------------|--------|----------------|
| 123     | 2023-01-01 08:00:00 | click  | 2023-01-01     |
| 123     | 2023-01-01 09:00:00 | view   | 2023-01-01     |
```
Very high volume, 1 row per event.

---

Daily aggregating what's above often gives you:  
```
| user_id | action_cnt | date_partition |
|---------|------------|----------------|
| 123     | 10         | 2023-01-01     |
| 123     | 15         | 2023-01-02     |
```
Medium-sized volume, 1 row per day.

---

**Reduced fact** takes this one step further:  
```
| user_id | array_of_action_cnts | month_start_partition |
|---------|-----------------------|------------------------|
| 123     | {10, 15, 20, 5}       | 2023-01-01             |
```
This reduces the data size by storing counts in arrays for each month or year, enabling faster queries for trends over time.

---

### Additional Fact Table Types with Examples:

**1. Snapshot Fact Table:**  
Captures the state of metrics at a specific point in time (e.g., daily balance).  
```
| account_id | balance | snapshot_date |
|------------|---------|---------------|
| 001        | 500.00  | 2023-01-01    |
| 001        | 550.00  | 2023-01-02    |
```

**2. Accumulating Snapshot Fact Table:**  
Tracks progress through a defined process (e.g., order lifecycle).  
```
| order_id | placed_date | shipped_date | delivered_date |
|----------|-------------|--------------|----------------|
| 1001     | 2023-01-01  | 2023-01-03   | 2023-01-05     |
```

**3. Factless Fact Table:**  
Captures events without direct numeric measures (e.g., student attendance).  
```
| student_id | event_date  | event_type |
|------------|-------------|------------|
| 456        | 2023-01-01  | attended   |
| 456        | 2023-01-02  | skipped    |
```

---

